{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bca1b34-d856-47ef-885c-b700c7d6cf77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PA3 - LoRA implementation on SmolLM\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, you will learn how to integrate LoRA (Low-Rank Adapters) into the SmolLM model you implemented in PA2. Before starting working on this notebook, please make sure to go through the README.md provided as it will intoduce to the concepts relevant to the assignment.\n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Follow along with the notebook, filling out the necessary code where instructed.\n",
    "\n",
    "- <span style=\"color: red;\">Read the Submission Instructions, Plagiarism Policy, and Late Days Policy in the attached PDF.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Make sure to run all cells for credit.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Do not remove any pre-written code.</span>\n",
    "\n",
    "- <span style=\"color: red;\">You must attempt all parts.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "797588d2-98aa-4950-a7dc-0bb91cbfc6ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad365a1e-e6b6-4d67-b6c3-fda645630c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-31T08:03:38.391087Z",
     "iopub.status.busy": "2025-03-31T08:03:38.390796Z",
     "iopub.status.idle": "2025-03-31T08:03:38.395770Z",
     "shell.execute_reply": "2025-03-31T08:03:38.394925Z",
     "shell.execute_reply.started": "2025-03-31T08:03:38.391068Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 23:11:11.152519: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745104271.167834    1618 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745104271.172304    1618 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-19 23:11:11.188850: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "# For tokenization and dataset loading\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3d3d197-fc6a-49b9-b52d-f8becf439593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Initializing device here for future use if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09aea4f3-6bda-402e-a3ea-59b6fbd9c6e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-31T07:51:19.117587Z",
     "iopub.status.busy": "2025-03-31T07:51:19.117117Z",
     "iopub.status.idle": "2025-03-31T07:51:19.166963Z",
     "shell.execute_reply": "2025-03-31T07:51:19.165803Z",
     "shell.execute_reply.started": "2025-03-31T07:51:19.117564Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"{device = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f2d798-e16a-484c-bfdc-8f509a671f29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### The implementation for the SmolLM model from PA2 has been added below for your convenience. You just need to run the next 5 cells in order to define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81aa6c1e-e580-4a17-a779-705a0bf28749",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-31T07:51:22.501615Z",
     "iopub.status.busy": "2025-03-31T07:51:22.501295Z",
     "iopub.status.idle": "2025-03-31T07:51:22.505882Z",
     "shell.execute_reply": "2025-03-31T07:51:22.505153Z",
     "shell.execute_reply.started": "2025-03-31T07:51:22.501589Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass \n",
    "class smolConfig:\n",
    "    vocab_size = 49152\n",
    "    hidden_size = 576\n",
    "    intermediate_size = 1536\n",
    "    num_hidden_layers = 30\n",
    "    num_heads = 9\n",
    "    kv_heads = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dae96d9-b766-48c2-81e2-eda1cb8a461a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-31T07:51:23.959935Z",
     "iopub.status.busy": "2025-03-31T07:51:23.959547Z",
     "iopub.status.idle": "2025-03-31T07:51:23.973791Z",
     "shell.execute_reply": "2025-03-31T07:51:23.972948Z",
     "shell.execute_reply.started": "2025-03-31T07:51:23.959898Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"\n",
    "    Helper function to rotate the left half of a tensor along its final dimension.\n",
    "    \"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"\n",
    "    Applies RoPE on the query and key tensors.\n",
    "    \"\"\"\n",
    "    cos, sin = cos.to(q.device), sin.to(q.device)\n",
    "\n",
    "    # Unsqueexzing to enable broadcasting\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    \n",
    "    return q_embed, k_embed\n",
    "\n",
    "class RotaryEmbedder(nn.Module):\n",
    "    def __init__(self, dim, base):\n",
    "        super().__init__()\n",
    "        # Precompute frequency for sine/cosine embeddings\n",
    "        self.freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        # Generate positions (sequence indices) for the input\n",
    "        pos = torch.arange(x.shape[-2], dtype=torch.long)\n",
    "        # Compute angles for sine and cosine embeddings\n",
    "        angles = torch.einsum(\"p,f->pf\", pos.float(), self.freq).unsqueeze(dim=0)\n",
    "        # Duplicate angles for sine and cosine embeddings\n",
    "        emb = torch.cat((angles, angles), dim=-1)\n",
    "        # Return cosine and sine components of the positional embeddings\n",
    "        return emb.cos(), emb.sin()\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Model dimensions and attention configurations\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = config.hidden_size // self.num_heads\n",
    "        self.kv_heads = config.kv_heads  # Number of key-value heads\n",
    "        self.rope_theta = 10000.0  # Scaling factor for rotary embeddings\n",
    "\n",
    "        # Linear projections for queries, keys, values, and output\n",
    "        self.q_proj = nn.Linear(config.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(config.hidden_size, self.kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.hidden_size, self.kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n",
    "\n",
    "        # Rotary embedding generator\n",
    "        self.rotary_emb = RotaryEmbedder(base=self.rope_theta, dim=self.head_dim)\n",
    "\n",
    "    def _repeat_kv(self, x, n_rep):\n",
    "        batch, num_key_value_heads, slen, head_dim = x.shape\n",
    "        # Expand the number of key-value heads by repeating them\n",
    "        x = x[:, :, None, :, :].expand(\n",
    "            batch, num_key_value_heads, n_rep, slen, head_dim\n",
    "        )\n",
    "        # Reshape to align with the expected multi-head attention format\n",
    "        return x.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, attention_mask=None):\n",
    "        # Input dimensions: (batch_size, seq_len, hidden_size)\n",
    "        b, q, _ = x.size()\n",
    "\n",
    "        # Project input hidden states into queries, keys, and values\n",
    "        q_states = self.q_proj(x)\n",
    "        k_states = self.k_proj(x)\n",
    "        v_states = self.v_proj(x)\n",
    "\n",
    "        # Reshape and transpose for multi-head attention\n",
    "        q_states = q_states.view(b, q, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k_states = k_states.view(b, q, self.kv_heads, self.head_dim).transpose(1, 2)\n",
    "        v_states = v_states.view(b, q, self.kv_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute rotary positional embeddings\n",
    "        cos, sin = self.rotary_emb(q_states)\n",
    "        cos = cos.to(q_states.device)\n",
    "        sin = sin.to(q_states.device)\n",
    "        # Apply positional embeddings to queries and keys\n",
    "        q_states, k_states = apply_rotary_pos_emb(q_states, k_states, cos, sin)\n",
    "\n",
    "        # Repeat key and value tensors to match the number of query heads\n",
    "        __kv_groups = self.num_heads // self.kv_heads\n",
    "        k_states = self._repeat_kv(k_states, __kv_groups)\n",
    "        v_states = self._repeat_kv(v_states, __kv_groups)\n",
    "\n",
    "        # Compute attention scores (scaled dot-product attention)\n",
    "        attn_weights = torch.matmul(q_states, k_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # Add attention mask (e.g., for causal or padding masking)\n",
    "        attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        # Normalize attention weights using softmax\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "        # Apply dropout to attention weights\n",
    "        attn_weights = nn.functional.dropout(attn_weights, 0)\n",
    "\n",
    "        # Compute attention output\n",
    "        attn_output = torch.matmul(attn_weights, v_states)\n",
    "        # Reshape and transpose back to original format\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.reshape(b, q, -1)\n",
    "\n",
    "        # Project the attention output back to the hidden size\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        # Return the final attention output\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8627a1ea-ceba-4cc3-84bc-44594c7e6667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-31T07:51:27.748636Z",
     "iopub.status.busy": "2025-03-31T07:51:27.748346Z",
     "iopub.status.idle": "2025-03-31T07:51:27.757156Z",
     "shell.execute_reply": "2025-03-31T07:51:27.756222Z",
     "shell.execute_reply.started": "2025-03-31T07:51:27.748615Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        This is the Root Mean Square Normalisation class.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))  # Learnable scaling factor\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate variance along the last dimension (hidden size)\n",
    "        variance = x.pow(2).mean(-1, keepdim=True)\n",
    "\n",
    "        # Normalize and scale\n",
    "        x = x * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size):\n",
    "        \"\"\"\n",
    "        This is the gated MLP from the LLaMa architecture. Here we use the SiLU acitvation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.activation = nn.modules.activation.SiLU()\n",
    "\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.activation(self.gate_proj(x)) * self.up_proj(x))\n",
    "        return down_proj\n",
    "    \n",
    "class LlamaDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        This is the Llama decoder block.\n",
    "        \"\"\"\n",
    "        # Self Attention Module\n",
    "        self.self_attn = GroupedQueryAttention(config)\n",
    "\n",
    "        # FFN Module\n",
    "        self.mlp = MLP(hidden_size=config.hidden_size, intermediate_size=config.intermediate_size)\n",
    "\n",
    "        # Pre Attention and Post Attention normalisation\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, eps=1e-05)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=1e-05)\n",
    "\n",
    "    def forward(self, x, attention_mask):\n",
    "        # Skip connection cache\n",
    "        \n",
    "        residual = x\n",
    "\n",
    "        # Pre-attention normalisation\n",
    "        x = self.input_layernorm(x)\n",
    "\n",
    "        # A causal attention mask (i.e., decoder can only look at tokens that it has generated thus far)\n",
    "        attention_mask = torch.triu(torch.full((attention_mask.shape[-1], attention_mask.shape[-1]),\n",
    "                                               fill_value=float('-inf')), diagonal=1)\n",
    "        \n",
    "        attention_mask = attention_mask.to(x.device)\n",
    "\n",
    "        # Self-attention block\n",
    "        x = self.self_attn(x=x,attention_mask=attention_mask)\n",
    "        x += residual\n",
    "\n",
    "        # Skip connection cache for MLP\n",
    "        residual = x\n",
    "\n",
    "        # Pre-MLP normalisation\n",
    "        x = self.post_attention_layernorm(x)\n",
    "\n",
    "        # MLP block\n",
    "        x = self.mlp(x)\n",
    "        x += residual\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c0a32f1-a599-4c42-a57f-b9f7dcf54be8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-31T07:51:30.099323Z",
     "iopub.status.busy": "2025-03-31T07:51:30.098923Z",
     "iopub.status.idle": "2025-03-31T07:51:30.106198Z",
     "shell.execute_reply": "2025-03-31T07:51:30.105377Z",
     "shell.execute_reply.started": "2025-03-31T07:51:30.099289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class smolModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # embedding layer which maps each token to a vector embedding\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            num_embeddings=config.vocab_size,\n",
    "            embedding_dim=config.hidden_size\n",
    "        )\n",
    "\n",
    "        # Stack of decoder layers (LlamaDecoder) defined by the configuration\n",
    "        self.layers = nn.ModuleList([\n",
    "            LlamaDecoder(config) for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "\n",
    "        # RMSNorm: final layer normalization applied to hidden states\n",
    "        self.norm = RMSNorm(config.hidden_size, eps=1e-05)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        inputs_embeds = self.embed_tokens(input_ids)\n",
    "        x = inputs_embeds\n",
    "\n",
    "        # Pass embeddings through each decoder layer\n",
    "        for i, decoder_layer in enumerate(self.layers):\n",
    "            layer_outputs = decoder_layer(\n",
    "                x,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            x = layer_outputs\n",
    "\n",
    "        # Final normalisation\n",
    "        x = self.norm(x)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "class smolLM(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the Language Model. \n",
    "    It passes the embeddings from the SmolLM backbone into a LM head.\n",
    "    The LM head generates logits over the space of the entire vocabulary for next word prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # SmolLM backbone which generates the contextualised embeddings for the input tokens\n",
    "        self.model = smolModel(config)\n",
    "        # The LM head which maps embeddings to logits over the vocabulary\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        # weights between LM head and the token_embedding layer are shared in the SmolLM architecture\n",
    "        self.tie_weights()\n",
    "\n",
    "    def tie_weights(self):\n",
    "        # lm_head shares weights with the embedding layer\n",
    "        self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Input tokens are passed to the SmolLM backbone\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        # embeddings corresponding to each input token => (batch_size, seq_len, emb_dim)\n",
    "        x = outputs\n",
    "\n",
    "        # pass the embeddings through the LM head\n",
    "        logits = self.lm_head(x).float()\n",
    "        return {'logits': logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9a95d62-e346-49a1-8fe4-687f893dfa73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-31T07:51:33.910554Z",
     "iopub.status.busy": "2025-03-31T07:51:33.910266Z",
     "iopub.status.idle": "2025-03-31T07:51:33.915953Z",
     "shell.execute_reply": "2025-03-31T07:51:33.914906Z",
     "shell.execute_reply.started": "2025-03-31T07:51:33.910534Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def __generate(model, inputs, num_tokens, tokenizer, max_length=50):\n",
    "    \"\"\"\n",
    "    A basic greedy approach for text generation.\n",
    "    \"\"\"\n",
    "    collect = []\n",
    "    for _ in range(num_tokens):\n",
    "        output = model(**inputs)\n",
    "        output_id = torch.argmax(output['logits'][0, -1]).item()\n",
    "        collect.append(output_id)\n",
    "        if output_id == tokenizer.eos_token_id or len(collect) >= max_length:\n",
    "            break\n",
    "        # Update input_ids and attention_mask\n",
    "        new_token = torch.tensor([output_id], device=inputs['input_ids'].device)\n",
    "        inputs['input_ids'] = torch.cat([inputs['input_ids'][0], new_token]).unsqueeze(0)\n",
    "        inputs['attention_mask'] = F.pad(inputs['attention_mask'], (0, 1), value=1)\n",
    "    return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(collect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6313e4df-90b9-40b8-97d7-43498135c4f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Understanding the Problem\n",
    "\n",
    "In this assignment, we'll implement LoRA, which allows efficient fine-tuning by adding low-rank decomposition matrices to specific weight matrices in the model. For each targeted weight matrix $W \\in \\mathbb{R}^{d \\times k}$, we'll create two smaller matrices $A \\in \\mathbb{R}^{r \\times k}$ and $B \\in \\mathbb{R}^{d \\times r}$ where $r \\ll \\min(d, k)$.\n",
    "\n",
    "The key equation is:\n",
    "$W' = W + \\frac{\\alpha}{r}BA$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5338c529-5815-4680-843c-eb7480a9c736",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Base LoRA Implementation\n",
    "First, we'll implement a generic LoRA module that can be applied to any linear layer in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0977c60f-b5f3-4430-a0ab-982df2890af3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-31T07:51:38.860139Z",
     "iopub.status.busy": "2025-03-31T07:51:38.859810Z",
     "iopub.status.idle": "2025-03-31T07:51:38.865906Z",
     "shell.execute_reply": "2025-03-31T07:51:38.865004Z",
     "shell.execute_reply.started": "2025-03-31T07:51:38.860110Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of a LoRA layer - a low-rank adaptation of a weight matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, rank=8, alpha=16, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Initialize a LoRA layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # scaling factor\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "        # matrix A\n",
    "        self.A = nn.Parameter(torch.rand((rank, out_features)))\n",
    "\n",
    "        # matrix B\n",
    "        self.B = nn.Parameter(torch.zeros((in_features, rank)))\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        h = x @ self.B @ self.A\n",
    "        output = h * self.scaling\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9d1355e-85be-47cc-b578-71d2625aa740",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Question 1**: Why is it important to initialize matrix B with zeros? How does this affect training at the beginning?\n",
    "\n",
    "**Ans**: $\\newline$\n",
    "We initialize the B matrix with zeros to ensure the fine-tuning begins exactly like the original model. $\\newline$\n",
    "When using LoRA, we have $\\Delta W$ in $W_{LoRA} = W_{orig}$, where $\\Delta W = BA$. $\\newline$\n",
    "By setting B to zero initially, we guaranteee that $\\Delta W = 0$ at the start of training, $\\newline$\n",
    "preventing any unwanted perturbations to the model's behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92e281aa-e3b4-4169-ae5e-824cdfcf6b4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### LoRA-Enhanced Linear Layer\n",
    "\n",
    "Now we'll create a wrapper for linear layers that incorporates LoRA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18590828-8ce7-498a-93aa-c4a05c2c6ec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-31T07:51:42.216823Z",
     "iopub.status.busy": "2025-03-31T07:51:42.216505Z",
     "iopub.status.idle": "2025-03-31T07:51:42.222384Z",
     "shell.execute_reply": "2025-03-31T07:51:42.221600Z",
     "shell.execute_reply.started": "2025-03-31T07:51:42.216797Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    A linear layer with LoRA adaptation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, linear_layer, rank=8, alpha=16, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Initialize a LoRA-adapted linear layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # original linear layer\n",
    "        self.linear = linear_layer\n",
    "\n",
    "        # freeze the weights of the original layer\n",
    "        for param in self.linear.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # input and output dimensions from the linear layer\n",
    "        in_features = self.linear.in_features\n",
    "        out_features = self.linear.out_features\n",
    "\n",
    "        # create the LoRA adaptation layer\n",
    "        self.lora = LoRALayer(in_features, out_features, rank, alpha, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.linear(x) + self.lora(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d3d84fc-df06-437d-9b51-1506af2d64e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Applying LoRA to SmolLM\n",
    "\n",
    "**LoRA Integration Strategy**\\\n",
    "We need to decide which weights in our model should be adapted with LoRA. In transformers, typical targets include:\n",
    "- Query, Key, Value projections in attention layers\n",
    "- Output projections in attention layers\n",
    "- Up/down projections in feed-forward networks\n",
    "\n",
    "Implement the function to add LoRA to specific linear layers in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffa3008d-45da-4e4e-b1fc-8c8084239c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-31T07:51:45.301595Z",
     "iopub.status.busy": "2025-03-31T07:51:45.301258Z",
     "iopub.status.idle": "2025-03-31T07:51:45.307586Z",
     "shell.execute_reply": "2025-03-31T07:51:45.306733Z",
     "shell.execute_reply.started": "2025-03-31T07:51:45.301566Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_lora_to_model(model, target_modules=None, rank=8, alpha=16, dropout=0.0):\n",
    "    \"\"\"\n",
    "    Add LoRA adapters to target modules in the model.\n",
    "\n",
    "    Returns:\n",
    "        Model with LoRA adapters\n",
    "    \"\"\"\n",
    "    model_with_lora = model\n",
    "\n",
    "    for param in model_with_lora.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for name, module in model_with_lora.named_modules():\n",
    "        n = name.split('.')[-1]\n",
    "        if (n in target_modules) and isinstance(module, nn.Linear):\n",
    "            parent_name, child_name = name.rsplit('.', 1)\n",
    "            parent_module = model_with_lora.get_submodule(parent_name)\n",
    "            lora_linear = LoRALinear(module, rank, alpha, dropout)\n",
    "            setattr(parent_module, child_name, lora_linear)\n",
    "\n",
    "    return model_with_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbe09845-4672-4791-82af-9d925b48fbdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Initializing the Base and LoRA models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf86a4c5-9429-4675-aff5-7e6762abd6d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-31T07:51:47.259792Z",
     "iopub.status.busy": "2025-03-31T07:51:47.259507Z",
     "iopub.status.idle": "2025-03-31T07:52:04.498219Z",
     "shell.execute_reply": "2025-03-31T07:52:04.497523Z",
     "shell.execute_reply.started": "2025-03-31T07:51:47.259771Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589f305b62dc4e7c89ac507c604f1227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/724 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2c88f5acd7453198591ae903cc6aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/538M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c78c9bebc74bfe9813ce3fdab7dc94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = smolConfig()\n",
    "base_model = smolLM(config)\n",
    "checkpoint = \"HuggingFaceTB/SmolLM-135M\"\n",
    "reference_model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "base_model.load_state_dict(reference_model.state_dict(), strict=False)\n",
    "\n",
    "target_modules = [\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\",\n",
    "    \"gate_proj\",\n",
    "]\n",
    "\n",
    "## DO NOT CHANGE THIS\n",
    "lora_model = add_lora_to_model(\n",
    "    base_model,\n",
    "    target_modules=target_modules,\n",
    "    rank=4,\n",
    "    alpha=8,\n",
    "    dropout=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65d07e49-f762-483b-8014-16d2e754401d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parameter Analysis\n",
    "\n",
    "Let's compare the parameter counts between the original model and the LoRA-enhanced version. Implement the parameter counting and analysis function.\n",
    "\n",
    "You should see that the % of trainable parameters in our `lora_model` should be <1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a642b86d-e9d1-46cf-a3ce-1ff8b0d1fb96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-31T07:52:09.233974Z",
     "iopub.status.busy": "2025-03-31T07:52:09.233692Z",
     "iopub.status.idle": "2025-03-31T07:52:09.251907Z",
     "shell.execute_reply": "2025-03-31T07:52:09.250906Z",
     "shell.execute_reply.started": "2025-03-31T07:52:09.233953Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters in Original Model: 135736128\n",
      "Trainable Parameters in LoRA Model: 1221120\n",
      "% of trainable parameters: 0.90%\n",
      "\n",
      "LoRA Parameters in each layer:\n",
      "q_proj: 138240\n",
      "k_proj: 92160\n",
      "v_proj: 92160\n",
      "o_proj: 138240\n",
      "up_proj: 253440\n",
      "down_proj: 253440\n",
      "gate_proj: 253440\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model: nn.Module, only_trainable: bool = False):\n",
    "    \"\"\"\n",
    "    Count the number of parameters in a model.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        only_trainable: If True, count only trainable parameters\n",
    "\n",
    "    Returns:\n",
    "        Number of parameters\n",
    "    \"\"\"\n",
    "    if only_trainable:\n",
    "            return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "def analyze_parameters(original_model: nn.Module, lora_model: nn.Module):\n",
    "    \"\"\"\n",
    "    Analyze parameter counts between original and LoRA-adapted models.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with parameter statistics\n",
    "    \"\"\"\n",
    "\n",
    "    total_params = count_parameters(original_model)\n",
    "    trainable_params = count_parameters(lora_model, only_trainable=True)\n",
    "\n",
    "    # calculate parameter savings\n",
    "    param_percent = (trainable_params / total_params) * 100\n",
    "\n",
    "    # count parameters by layer type\n",
    "    lora_params_by_type = {}\n",
    "    for name, module in lora_model.named_modules():\n",
    "        if isinstance(module, LoRALayer):\n",
    "            # extract the module type from the name\n",
    "            parts = name.split(\".\")\n",
    "            module_type = next(\n",
    "                (\n",
    "                    p\n",
    "                    for p in parts\n",
    "                    if any(\n",
    "                        t in p\n",
    "                        for t in [\n",
    "                            \"q_proj\",\n",
    "                            \"k_proj\",\n",
    "                            \"v_proj\",\n",
    "                            \"o_proj\",\n",
    "                            \"up_proj\",\n",
    "                            \"down_proj\",\n",
    "                            \"gate_proj\",\n",
    "                        ]\n",
    "                    )\n",
    "                ),\n",
    "                \"other\",\n",
    "            )\n",
    "\n",
    "            # count parameters in this LoRA layer\n",
    "            params = sum(p.numel() for p in module.parameters())\n",
    "\n",
    "            # add to the count by type\n",
    "            if module_type in lora_params_by_type:\n",
    "                lora_params_by_type[module_type] += params\n",
    "            else:\n",
    "                lora_params_by_type[module_type] = params\n",
    "\n",
    "    stats =  {\n",
    "        \"total_params\": total_params,\n",
    "        \"trainable_params\": trainable_params,\n",
    "        \"param_percent\": param_percent,\n",
    "        \"params_by_type\": lora_params_by_type,\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "stats = analyze_parameters(base_model, lora_model)\n",
    "\n",
    "print(f\"Total Parameters in Original Model: {stats['total_params']}\")\n",
    "print(f\"Trainable Parameters in LoRA Model: {stats['trainable_params']}\")\n",
    "print(f\"% of trainable parameters: {stats['param_percent']:.2f}%\")\n",
    "print()\n",
    "print(f\"LoRA Parameters in each layer:\")\n",
    "for k, v in stats['params_by_type'].items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58ba00e6-98cb-47d0-9e81-762edbff2741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Fine-tuning with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aea34df9-2d3d-45e4-86b2-a761ffb8d935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Dataset Preparation\n",
    "Let's set up a small dataset for fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "773b0cf9-d94d-441b-8323-2ab536d5b987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-31T07:52:15.362323Z",
     "iopub.status.busy": "2025-03-31T07:52:15.361994Z",
     "iopub.status.idle": "2025-03-31T07:52:15.368752Z",
     "shell.execute_reply": "2025-03-31T07:52:15.367859Z",
     "shell.execute_reply.started": "2025-03-31T07:52:15.362296Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(\n",
    "    tokenizer,\n",
    "    dataset_name=\"databricks/databricks-dolly-15k\",\n",
    "    subset=None,\n",
    "    max_samples=500,\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepare a dataset for fine-tuning.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: Tokenizer to use\n",
    "        dataset_name: HuggingFace dataset name\n",
    "        subset: Dataset subset (if applicable)\n",
    "        max_samples: Maximum number of samples to use\n",
    "\n",
    "    Returns:\n",
    "        Processed dataset ready for training\n",
    "    \"\"\"\n",
    "    # load dataset\n",
    "    if subset:\n",
    "        dataset = load_dataset(dataset_name, subset)\n",
    "    else:\n",
    "        dataset = load_dataset(dataset_name)\n",
    "\n",
    "    train_data = (\n",
    "        dataset[\"train\"]\n",
    "        .shuffle(seed=42)\n",
    "        .select(range(min(max_samples, len(dataset[\"train\"]))))\n",
    "    )\n",
    "\n",
    "    train_val_split = train_data.train_test_split(test_size=0.2, seed=42)\n",
    "    train_data = train_val_split[\"train\"]\n",
    "    val_data = train_val_split[\"test\"]\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        tokenized = tokenizer(examples[\"instruction\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "        return tokenized\n",
    "\n",
    "    train_tokenized = train_data.map(\n",
    "        tokenize_function, batched=True, remove_columns=train_data.column_names\n",
    "    )\n",
    "    val_tokenized = val_data.map(\n",
    "        tokenize_function, batched=True, remove_columns=val_data.column_names\n",
    "    )\n",
    "\n",
    "    train_tokenized.set_format(\"torch\")\n",
    "    val_tokenized.set_format(\"torch\")\n",
    "\n",
    "    train_dataloader = DataLoader(train_tokenized, batch_size=8, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_tokenized, batch_size=8)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aef345b8-c14a-4b96-8ad0-5d1fe2d501b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Initialzing our Tokenizer and Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8efc39a-73d8-4f18-8c5d-8a5a407dbdf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-31T07:52:17.814073Z",
     "iopub.status.busy": "2025-03-31T07:52:17.813749Z",
     "iopub.status.idle": "2025-03-31T07:52:25.982440Z",
     "shell.execute_reply": "2025-03-31T07:52:25.981598Z",
     "shell.execute_reply.started": "2025-03-31T07:52:17.814046Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1f1a7d96c14a1e9a743d9055a15449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468fd4332549458a818840d9e267cb08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e483f786114d40a6dfd10221c2b0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b1bb4e827c49f586fbac601c9be31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc0a3cfd013468c806f19f9944e72d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/lib/dbruntime/huggingface_patches/datasets.py:49: UserWarning: The cache_dir for this dataset is /tmp/.hf.data.cache, which is not a persistent path.Therefore, if/when the cluster restarts, the downloaded dataset will be lost.The persistent storage options for this workspace/cluster config are: [DBFS].Please update either `cache_dir` or the environment variable `HF_DATASETS_CACHE`to be under one of the following root directories: ['/dbfs/']\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67de830692d4e3faded1b7767ef43c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/8.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/lib/dbruntime/huggingface_patches/datasets.py:18: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be7a472191146a08ad3d994be9186c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "databricks-dolly-15k.jsonl:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6887a35cf17f4344ad0efb529ed35975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e3f4c4694b4703abe1535ce4b65bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6197b542fbb241c983ffbe107aadd4fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# the tokenizer does not have a defined padding token, so we initialize our own as the [EOS] token.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_dataloader, val_dataloader = prepare_dataset(tokenizer=tokenizer,  dataset_name=\"databricks/databricks-dolly-15k\", max_samples=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62f6903d-b33f-45d2-8a85-5c51cf69f8dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**We can test our base model to ensure it's working correctly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab4793cc-f559-4645-a01b-cf565e638f7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-31T07:52:28.153861Z",
     "iopub.status.busy": "2025-03-31T07:52:28.153577Z",
     "iopub.status.idle": "2025-03-31T07:52:37.493377Z",
     "shell.execute_reply": "2025-03-31T07:52:37.492458Z",
     "shell.execute_reply.started": "2025-03-31T07:52:28.153840Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Output generated====================\n",
      "The future of AI is  bright, but it’s not without its challenges. One of the biggest challenges is the lack of regulation and oversight. AI systems are often developed and deployed without the necessary safeguards in place to ensure they are safe and ethical. This lack of regulation can\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The future of AI is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "out = __generate(base_model, inputs, num_tokens=100, tokenizer=tokenizer)\n",
    "\n",
    "print('=='*10 + f' Output generated' + '=='*10)\n",
    "print(prompt + ' ' + out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a63c6c90-077c-4f11-a474-551cf0be423b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Training Loop\n",
    "The training function for our model with LoRA adapters has been implemented below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5511bdc2-4084-479d-9235-7034357b15e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-31T08:05:58.185497Z",
     "iopub.status.busy": "2025-03-31T08:05:58.185212Z",
     "iopub.status.idle": "2025-03-31T08:05:58.197454Z",
     "shell.execute_reply": "2025-03-31T08:05:58.196618Z",
     "shell.execute_reply.started": "2025-03-31T08:05:58.185478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_lora(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    optimizer,\n",
    "    epochs=3,\n",
    "    device=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a model with LoRA adapters.\n",
    "\n",
    "    Args:\n",
    "        model: LoRA-adapted model\n",
    "        train_dataloader: Training data\n",
    "        val_dataloader: Validation data\n",
    "        optimizer: PyTorch optimizer\n",
    "        epochs: Number of training epochs\n",
    "        device: Device to train on\n",
    "\n",
    "    Returns:\n",
    "        Training history\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_perplexity\": [],\n",
    "        \"val_perplexity\": [],\n",
    "    }\n",
    "\n",
    "    # add scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs[\"logits\"]\n",
    "\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "            shift_attention_mask = attention_mask[:, 1:].contiguous()\n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "            )\n",
    "\n",
    "            loss = loss.view(shift_labels.size())\n",
    "            loss = (loss * shift_attention_mask).sum() / shift_attention_mask.sum()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            progress_bar.set_postfix({\"train_loss\": loss.item()})\n",
    "\n",
    "        avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "        avg_train_perplexity = torch.exp(torch.tensor(avg_train_loss)).item()\n",
    "\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "\n",
    "        progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in progress_bar:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs[\"logits\"]\n",
    "\n",
    "                shift_logits = logits[:, :-1, :].contiguous()\n",
    "                shift_labels = labels[:, 1:].contiguous()\n",
    "                shift_attention_mask = attention_mask[:, 1:].contiguous()\n",
    "\n",
    "                loss_fct = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "                loss = loss_fct(\n",
    "                    shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "                )\n",
    "\n",
    "                loss = loss.view(shift_labels.size())\n",
    "                loss = (loss * shift_attention_mask).sum() / shift_attention_mask.sum()\n",
    "\n",
    "                val_losses.append(loss.item())\n",
    "                progress_bar.set_postfix({\"val_loss\": loss.item()})\n",
    "\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        avg_val_perplexity = torch.exp(torch.tensor(avg_val_loss)).item()\n",
    "\n",
    "        history[\"train_loss\"].append(avg_train_loss)\n",
    "        history[\"val_loss\"].append(avg_val_loss)\n",
    "        history[\"train_perplexity\"].append(avg_train_perplexity)\n",
    "        history[\"val_perplexity\"].append(avg_val_perplexity)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs} - \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f}, Train Perplexity: {avg_train_perplexity:.4f}, \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}, Val Perplexity: {avg_val_perplexity:.4f}\"\n",
    "        )\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2059a83d-7331-451b-b746-ccd4db07ce09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Training our Model. \n",
    "**DO NOT MODIFY THE HYPERPARAMETERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aed2ac9-bdc7-442d-b273-fd7ea7290377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-30T15:29:00.870960Z",
     "iopub.status.busy": "2025-03-30T15:29:00.870670Z",
     "iopub.status.idle": "2025-03-30T15:49:59.277715Z",
     "shell.execute_reply": "2025-03-30T15:49:59.276749Z",
     "shell.execute_reply.started": "2025-03-30T15:29:00.870939Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2ee5fd4f374b0c84da9d59cef3d28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5 [Train]:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2f8df6e89941bda3ed62a0f17729ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5 [Val]:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 3.4715, Train Perplexity: 32.1851, Val Loss: 3.2260, Val Perplexity: 25.1784\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732136cb26e84f2cbbdb959f8806a670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5 [Train]:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94615363a70e4d5683550b347f9f6b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5 [Val]:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Train Loss: 3.1393, Train Perplexity: 23.0875, Val Loss: 3.1613, Val Perplexity: 23.6018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4499b0db57043a6a7e90fa9a3654c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5 [Train]:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6888b74a29164b41936058062b58c80a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5 [Val]:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Train Loss: 2.9659, Train Perplexity: 19.4116, Val Loss: 3.1393, Val Perplexity: 23.0868\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1fda6e1cbe4aa1859c19858cd5b488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5 [Train]:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a9b63e7cbd466f92c85b9a618b9b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5 [Val]:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Train Loss: 2.8838, Train Perplexity: 17.8828, Val Loss: 3.1466, Val Perplexity: 23.2559\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e389b2d58804014992cec8c743918b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5 [Train]:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc0e46347794479bc06c9b379f79bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5 [Val]:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Train Loss: 2.8072, Train Perplexity: 16.5632, Val Loss: 3.1421, Val Perplexity: 23.1528\n"
     ]
    }
   ],
   "source": [
    "## DO NOT CHANGE THIS\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [p for p in lora_model.parameters() if p.requires_grad], lr=1e-4, weight_decay=0.01\n",
    ")\n",
    "\n",
    "history, trained_lora_model = train_lora(model=lora_model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, optimizer=optimizer, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "469f8c36-1023-4592-a492-58648ff87838",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Optional: You can save your trained model in case you decide to do the assignment in parts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14843792-2d91-4f9a-bde2-5aec03495910",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-30T15:51:28.256504Z",
     "iopub.status.busy": "2025-03-30T15:51:28.256181Z",
     "iopub.status.idle": "2025-03-30T15:51:29.086334Z",
     "shell.execute_reply": "2025-03-30T15:51:29.085292Z",
     "shell.execute_reply.started": "2025-03-30T15:51:28.256478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(trained_lora_model.state_dict(), \"/dbfs/lora_finetuned_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80a1087c-e98f-4823-a287-f1525f92bf1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Merging LoRA Weights for Inference\n",
    "For efficient inference, we can merge LoRA weights with the original weights.\n",
    "\n",
    "Implement the function `merge_lora_weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec32894b-827f-49f6-b68d-6c30e5e42bbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-31T07:55:24.905198Z",
     "iopub.status.busy": "2025-03-31T07:55:24.904886Z",
     "iopub.status.idle": "2025-03-31T07:55:24.911480Z",
     "shell.execute_reply": "2025-03-31T07:55:24.910608Z",
     "shell.execute_reply.started": "2025-03-31T07:55:24.905156Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def merge_lora_weights(model):\n",
    "    \"\"\"\n",
    "    Merge LoRA weights with original weights for efficient inference.\n",
    "\n",
    "    Args:\n",
    "        model: LoRA-adapted model\n",
    "\n",
    "    Returns:\n",
    "        Model with merged weights\n",
    "    \"\"\"\n",
    "    ## your code here\n",
    "\n",
    "    merged_model = model\n",
    "\n",
    "    for name, module in merged_model.named_modules():\n",
    "        if isinstance(module, LoRALinear):\n",
    "            with torch.no_grad():\n",
    "                W_delta = (module.lora.B @ module.lora.A).T * module.lora.scaling\n",
    "                module.linear.weight.data += W_delta\n",
    "            \n",
    "            parent_name, child_name = name.rsplit(\".\", 1)\n",
    "            parent_module = merged_model.get_submodule(parent_name)\n",
    "            setattr(parent_module, child_name, module.linear)\n",
    "    \n",
    "    return merged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a08bc90-99f6-4f54-8a81-b5b868aede65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-31T07:56:28.878718Z",
     "iopub.status.busy": "2025-03-31T07:56:28.878431Z",
     "iopub.status.idle": "2025-03-31T07:56:30.055127Z",
     "shell.execute_reply": "2025-03-31T07:56:30.054378Z",
     "shell.execute_reply.started": "2025-03-31T07:56:28.878695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Merge LoRA weights into the base model\n",
    "merged_model = merge_lora_weights(trained_lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7197be35-b6e2-412c-9461-1f0f36a1ddac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Optional: Save your merged model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "692a37c4-d6e4-453d-b4e1-8d8d1723463d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "torch.save(merged_model.state_dict(), \"/dbfs/merged_lora_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "785bfaae-cd7a-4601-9916-8cb9b2cb38c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Text Generation and Comparison\n",
    "Now let's compare text generation between models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef4fdee8-1340-4808-aeec-fa6e80aea507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Loading in the fully finetuned model.\n",
    "Instead of having you fully finetune the model, we are sharing the weights to make your life a little easier. First, we'll load in our fully finetuned model. The weights are accesible through the drive link [Finetuned Base Model Weights](https://drive.google.com/drive/folders/1eIflNAp9UE4Fm8ZrBAzjDPsOCs-s_O55?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63a1b47b-1c8f-48ea-9878-0584ba446f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-31T07:56:39.645633Z",
     "iopub.status.busy": "2025-03-31T07:56:39.645337Z",
     "iopub.status.idle": "2025-03-31T07:56:41.845043Z",
     "shell.execute_reply": "2025-03-31T07:56:41.844187Z",
     "shell.execute_reply.started": "2025-03-31T07:56:39.645612Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smolLM(\n",
       "  (model): smolModel(\n",
       "    (embed_tokens): Embedding(49152, 576)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoder(\n",
       "        (self_attn): GroupedQueryAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (rotary_emb): RotaryEmbedder()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (activation): SiLU()\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_base_model = smolLM(config)\n",
    "\n",
    "## add your model path here\n",
    "model_path = \"/dbfs/FileStore/full_finetuned_smolLM.pth\"\n",
    "\n",
    "# Load the finetuned weights into the base model\n",
    "finetuned_base_model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "\n",
    "# Set to eval mode for inference\n",
    "finetuned_base_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4ebc7db-1232-49c9-ad52-4aaddb960650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### We can now compare the fully finetuned and LoRA finetuned model to evaluate the effectiveness of using LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "854f0ed8-7680-4034-8d40-699cfc41a2e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-31T07:56:41.923160Z",
     "iopub.status.busy": "2025-03-31T07:56:41.922893Z",
     "iopub.status.idle": "2025-03-31T07:56:41.931922Z",
     "shell.execute_reply": "2025-03-31T07:56:41.930721Z",
     "shell.execute_reply.started": "2025-03-31T07:56:41.923139Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compare_generations(models, tokenizer, prompts, max_tokens=100):\n",
    "    \"\"\"\n",
    "    Compare text generation between different model versions.\n",
    "\n",
    "    Args:\n",
    "        models: Dictionary of models to compare\n",
    "        tokenizer: Tokenizer\n",
    "        prompts: List of prompts to test\n",
    "        max_tokens: Maximum tokens to generate\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with generation results\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    results = []\n",
    "\n",
    "    def calculate_perplexity(model, inputs):\n",
    "        \"\"\"\n",
    "        Computes perplexity for a given model and input.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs[\"logits\"]\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = inputs[\"input_ids\"][:, 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "            )\n",
    "            perplexity = torch.exp(loss).item()\n",
    "        return perplexity\n",
    "\n",
    "    for prompt in prompts:\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        prompt_results = {\"Prompt\": prompt}\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "\n",
    "            start_time = time.time()\n",
    "            output = __generate(\n",
    "                model, inputs.copy(), num_tokens=max_tokens, tokenizer=tokenizer\n",
    "            )\n",
    "            end_time = time.time()\n",
    "\n",
    "            perplexity = calculate_perplexity(model, inputs)\n",
    "\n",
    "            prompt_results[f\"{model_name} Perplexity\"] = perplexity\n",
    "\n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(f\"Generated: {output}\")\n",
    "            print(f\"Time: {end_time - start_time:.2f}s\")\n",
    "            print(f\"Perplexity: {perplexity:.4f}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        results.append(prompt_results)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4feba8b5-a92e-48d6-be46-774a761aa3fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2025-03-31T07:56:49.703445Z",
     "iopub.status.busy": "2025-03-31T07:56:49.703109Z",
     "iopub.status.idle": "2025-03-31T07:57:04.313930Z",
     "shell.execute_reply": "2025-03-31T07:57:04.313155Z",
     "shell.execute_reply.started": "2025-03-31T07:56:49.703409Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Once upon a time, in a distant galaxy,\n",
      "Model: Fully Finetuned Model\n",
      "Generated:  there was a kind-hearted alien named Zork. One day, while exploring the universe, Zork came across a planet called Zorbaheim. Excited to explore, Zork decided to visit the planet and start his new life.\n",
      "\n",
      "\n",
      "Time: 2.09s\n",
      "Perplexity: 5.9674\n",
      "--------------------------------------------------\n",
      "Model: LoRA Finetuned Model\n",
      "Generated:  lived a curious little girl named Lily. She was always eager to learn new things and explore the world around her. One day, she asked her mother, \"Mommy, what is a 'book'?\" Her mother smiled and replied, \"A\n",
      "Time: 1.76s\n",
      "Perplexity: 4.7975\n",
      "--------------------------------------------------\n",
      "Prompt: The future of artificial intelligence is\n",
      "Model: Fully Finetuned Model\n",
      "Generated:  very bright.<|endoftext|>\n",
      "Time: 0.16s\n",
      "Perplexity: 22.8368\n",
      "--------------------------------------------------\n",
      "Model: LoRA Finetuned Model\n",
      "Generated:  bright. AI is already being used in many industries, and it is likely to become even more prevalent in the future.\n",
      "What is the future of AI?\n",
      "The future of AI is bright. AI is already being used in many industries, and\n",
      "Time: 1.85s\n",
      "Perplexity: 15.9490\n",
      "--------------------------------------------------\n",
      "Prompt: A wise old wizard once said,\n",
      "Model: Fully Finetuned Model\n",
      "Generated:  \"The more you learn, the better you get!\" And so, our tale continues, with dragons, griffins, minotaurs, centaurs, and half-elf half-human creatures learning valuable life lessons from their experiences.<|endoftext|>\n",
      "Time: 2.12s\n",
      "Perplexity: 53.2328\n",
      "--------------------------------------------------\n",
      "Model: LoRA Finetuned Model\n",
      "Generated:  \"The most important thing is to always keep learning and growing.\"\n",
      "\n",
      "One day, while playing near the river, they found a shiny stone. It was so big that it could hold all their toys! Excitedly, they ran towards it. But\n",
      "Time: 1.82s\n",
      "Perplexity: 37.8092\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define models for comparison\n",
    "models = {\n",
    "    \"Fully Finetuned Model\": finetuned_base_model,\n",
    "    \"LoRA Finetuned Model\": merged_model,\n",
    "}\n",
    "\n",
    "# Define prompts to test\n",
    "prompts = [\n",
    "    \"Once upon a time, in a distant galaxy,\",\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"A wise old wizard once said,\",\n",
    "]\n",
    "\n",
    "# Run the comparison\n",
    "df_results = compare_generations(models, tokenizer, prompts, max_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8a4ca11-5e7d-41d8-a815-ffe62d0b8088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Compare the perplexity scores of the models**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f5eb27b-4dcb-4cde-abac-b7aed46aab10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒════╤══════════════════════════════════════════╤════════════════════════════════════╤═══════════════════════════════════╕\n",
      "│    │ Prompt                                   │   Fully Finetuned Model Perplexity │   LoRA Finetuned Model Perplexity │\n",
      "╞════╪══════════════════════════════════════════╪════════════════════════════════════╪═══════════════════════════════════╡\n",
      "│  0 │ Once upon a time, in a distant galaxy,   │                             5.9674 │                           4.79752 │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────┼───────────────────────────────────┤\n",
      "│  1 │ The future of artificial intelligence is │                            22.8368 │                          15.949   │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────┼───────────────────────────────────┤\n",
      "│  2 │ A wise old wizard once said,             │                            53.2328 │                          37.8092  │\n",
      "╘════╧══════════════════════════════════════════╧════════════════════════════════════╧═══════════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(df_results, headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86fa7cfc-4749-40d1-bec3-144deac71047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Analysis and Discussion\n",
    "For this section, analyze your results and answer the following questions:\n",
    "\n",
    "**Question 2:** How does LoRA performance compare to full fine-tuning? What are the tradeoffs? $\\newline$\n",
    "**Ans:** $\\newline$\n",
    "While LoRA provides a significant reduction in the number of trainable parameters, it creates information loss through matrix decomposition. $\\newline$\n",
    "Because the full weight matrix is reduced into smaller components, some important details can be lost. $\\newline$\n",
    "In case of LLMs though, the loss is often minimal becase of over-parameterization and redundancy in the original weights. $\\newline$\n",
    "\n",
    "\n",
    "**Question 3:** Which target modules benefit most from LoRA adaptation in SmolLM? $\\newline$\n",
    "**Ans:** $\\newline$\n",
    "The modules that benefit the most from LoRA in SmolLM are usually:\n",
    "1. Attention projections (q_proj, v_proj): These handle task-specific attention patterns and are key for aligning tokens meaningfully.\n",
    "2. Output projections (o_proj): These shape the final attention output and directly influence downstream performance.\n",
    "3. Feed-forward layers (up_proj, down_proj): Useful for adapting the hidden representations in task-specific ways.\n",
    "Of all attention components, k_proj tends to matter less since it's more about attention retrieval than representation learning.\n",
    "\n",
    "\n",
    "**Question 4:** How does rank value affect the quality of adaptation and the parameter count? $\\newline$\n",
    "**Ans:** $\\newline$\n",
    "The rank in LoRA controls the capacity of the low-rank update. $\\newline$\n",
    "A higher rank allows the model to capture more complex task-specific patterns but increases the number of trainable parameters. $\\newline$\n",
    "A lower rank reduces training cost and memory usage but loses information, underperforming on complex tasks. $\\newline$\n",
    "LoRA weight matrices A and B have dimensions ($r \\times k$) and ($d \\times r$) respectively. $\\newline$\n",
    "Reducing r reduces the parameters while increasing r increases the parameters. $\\newline$\n",
    "\n",
    "**Question 5:** What are the practical benefits of LoRA for deploying fine-tuned models? $\\newline$\n",
    "**Ans:**\n",
    "1. **Efficiency:** Only a few parameters are trained and stored, the rest are frozen. This allows for efficient use of computational resources.\n",
    "2. **Modularity:** LoRA adapters can we swapped in/out for task-specific purposes without affecting the base model.\n",
    "3. **Deployment Simplicity:** Only a small amount of LoRA weights (<%1) need to be transferred and stored for each task\n",
    "4. **Multi-tasking:** Can easily support multiple downstream tasks just by loading different LoRA adapters into the model.\n",
    "This makes LoRA favorable for multi-task inferencing and environments with strict resource constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d78a27cf-6339-4a53-bb3a-30dc743db664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Fin."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PA3",
   "widgets": {}
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 280292,
     "modelInstanceId": 259091,
     "sourceId": 303465,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
